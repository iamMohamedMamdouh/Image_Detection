import streamlit as st
from PIL import Image
from ultralytics import YOLO
import numpy as np
from gtts import gTTS
import tempfile
import os
from collections import Counter
import base64

st.set_page_config(page_title="Image Detection Model", layout="centered")
st.title("Image Detection Model")
st.markdown("Upload an image and see the result after the model analyzes it")

model = YOLO("yolov8n.pt")  # ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø±ÙÙˆØ¹ Ù…Ø¹ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø±Ø§Ø¨Ø·Ù‡ Ù…Ù† Hugging Face

translate = {
    "person": "Ø´Ø®Øµ",
    "car": "Ø³ÙŠØ§Ø±Ø©",
    "truck": "Ø´Ø§Ø­Ù†Ø©",
    "bus": "Ø£ØªÙˆØ¨ÙŠØ³",
    "bicycle": "Ø¯Ø±Ø§Ø¬Ø©",
    "motorcycle": "Ø¯Ø±Ø§Ø¬Ø© Ù†Ø§Ø±ÙŠØ©",
    "cat": "Ù‚Ø·Ø©",
    "dog": "ÙƒÙ„Ø¨",
    "traffic light": "Ø¥Ø´Ø§Ø±Ø© Ù…Ø±ÙˆØ±",
    "handbag": "Ø­Ù‚ÙŠØ¨Ø© ÙŠØ¯",
    "chair": "ÙƒØ±Ø³ÙŠ",
    "bird": "Ø·Ø§Ø¦Ø±",
    "boat": "Ù‚Ø§Ø±Ø¨",
    "backpack": "Ø­Ù‚ÙŠØ¨Ø© Ø¸Ù‡Ø±",
}

uploaded_file = st.file_uploader("Upload a picture", type=["jpg", "jpeg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    img_array = np.array(image)

    st.subheader("ğŸ“· Original Image:")
    st.image(image, use_container_width=True)

    result = model.predict(img_array, conf=0.3)[0]
    boxes = result.boxes.data
    class_names = result.names

    st.subheader("ğŸ§  Detected Image:")
    st.image(result.plot(), caption="âœ… Detected", use_container_width=True)

    detected_labels = [class_names[int(cls)] for *_, cls in boxes.tolist()]
    if detected_labels:
        counts = Counter(detected_labels)

        col1, col2 = st.columns([1, 3])
        with col1:
            lang_choice = st.selectbox("ğŸŒ", ["English", "Ø¹Ø±Ø¨ÙŠ"], index=0, label_visibility="collapsed")
        with col2:
            speak = st.button("ğŸ”Š Announce Results")

        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Øµ
        lines = []
        if lang_choice == "Ø¹Ø±Ø¨ÙŠ":
            for k, v in counts.items():
                arabic = translate.get(k, k)
                lines.append(f"{arabic} ÙˆØ§Ø­Ø¯" if v == 1 else f"{v} {arabic}")
            speak_text = "ØªÙ… Ø§ÙƒØªØ´Ø§Ù: " + "ØŒ ".join(lines)
        else:
            for k, v in counts.items():
                lines.append(f"{k}" if v == 1 else f"{v} {k}s")
            speak_text = "Detected: " + ", ".join(lines)

        display_text = "\n".join(f"- {line}" for line in lines)
        st.success("ğŸ“Š Results:\n" + display_text)

        if speak:
            try:
                tts = gTTS(text=speak_text, lang='ar' if lang_choice == "Ø¹Ø±Ø¨ÙŠ" else 'en')
                tmp_path = os.path.join(tempfile.gettempdir(), "speech.mp3")
                tts.save(tmp_path)

                with open(tmp_path, "rb") as audio_file:
                    audio_bytes = audio_file.read()
                    b64 = base64.b64encode(audio_bytes).decode()
                    audio_html = f"""
                        <audio autoplay style="display: none">
                        <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
                        </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)

                os.remove(tmp_path)
            except Exception as e:
                st.error(f"âš  Ø­ØµÙ„Øª Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ù†Ø·Ù‚: {str(e)}")
    else:
        st.warning("âŒ No objects detected.")
#/////////////////////////////////////////////////////////////////
# import streamlit as st
# from PIL import Image
# from ultralytics import YOLO
# import numpy as np
# from gtts import gTTS
# import tempfile
# import os
# from collections import Counter
# import base64
#
# st.set_page_config(page_title="Image Detection Model", layout="centered")
# st.title("Image Detection Model")
# st.markdown("Upload an image and see the result after the model analyzes it")
#
# model = YOLO("yolov8n.pt")
#
# translate = {
#     "person": "Ø´Ø®Øµ",
#     "car": "Ø³ÙŠØ§Ø±Ø©",
#     "truck": "Ø´Ø§Ø­Ù†Ø©",
#     "bus": "Ø£ØªÙˆØ¨ÙŠØ³",
#     "bicycle": "Ø¯Ø±Ø§Ø¬Ø©",
#     "motorcycle": "Ø¯Ø±Ø§Ø¬Ø© Ù†Ø§Ø±ÙŠØ©",
#     "cat": "Ù‚Ø·Ø©",
#     "dog": "ÙƒÙ„Ø¨",
#     "traffic light": "Ø¥Ø´Ø§Ø±Ø© Ù…Ø±ÙˆØ±",
#     "handbag": "Ø­Ù‚ÙŠØ¨Ø© ÙŠØ¯",
#     "chair": "ÙƒØ±Ø³ÙŠ",
#     "bird": "Ø·Ø§Ø¦Ø±",
#     "boat": "Ù‚Ø§Ø±Ø¨",
#     "backpack": "Ø­Ù‚ÙŠØ¨Ø© Ø¸Ù‡Ø±",
# }
#
# uploaded_file = st.file_uploader("Upload a picture", type=["jpg", "jpeg", "png"])
#
# if uploaded_file:
#     image = Image.open(uploaded_file).convert("RGB")
#     img_array = np.array(image)
#
#     st.subheader("ğŸ“· Original Image:")
#     st.image(image, use_container_width=True)
#
#     result = model.predict(img_array, conf=0.3)[0]
#     boxes = result.boxes.data
#     class_names = result.names
#
#     st.subheader("ğŸ§  Detected Image:")
#     st.image(result.plot(), caption="âœ… Detected", use_container_width=True)
#
#     detected_labels = [class_names[int(cls)] for *_, cls in boxes.tolist()]
#     if detected_labels:
#         counts = Counter(detected_labels)
#
#         col1, col2 = st.columns([1, 3])
#         with col1:
#             lang_choice = st.selectbox("ğŸŒ", ["English", "Ø¹Ø±Ø¨ÙŠ"], index=0, label_visibility="collapsed")
#         with col2:
#             speak = st.button("ğŸ”Š Announce Results")
#
#         # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Øµ
#         lines = []
#         if lang_choice == "Ø¹Ø±Ø¨ÙŠ":
#             for k, v in counts.items():
#                 arabic = translate.get(k, k)
#                 lines.append(f"{arabic} ÙˆØ§Ø­Ø¯" if v == 1 else f"{v} {arabic}")
#             speak_text = "ØªÙ… Ø§ÙƒØªØ´Ø§Ù: " + "ØŒ ".join(lines)
#         else:
#             for k, v in counts.items():
#                 lines.append(f"{k}" if v == 1 else f"{v} {k}s")
#             speak_text = "Detected: " + ", ".join(lines)
#
#         display_text = "\n".join(f"- {line}" for line in lines)
#         st.success("ğŸ“Š Results:\n" + display_text)
#
#         if speak:
#             try:
#                 tts = gTTS(text=speak_text, lang='ar' if lang_choice == "Ø¹Ø±Ø¨ÙŠ" else 'en')
#
#                 # Ù†Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙˆÙ†Ù‚ÙÙ„Ù‡ Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡
#                 tmp_path = os.path.join(tempfile.gettempdir(), "speech.mp3")
#                 tts.save(tmp_path)
#
#                 # ØªØ£ÙƒØ¯ Ù…Ù† Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…Ù„Ù Ù‚Ø¨Ù„ Ø§Ù„ÙØªØ­ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©
#                 with open(tmp_path, "rb") as audio_file:
#                     audio_bytes = audio_file.read()
#                     b64 = base64.b64encode(audio_bytes).decode()
#                     audio_html = f"""
#                         <audio autoplay
#                         style="display: none">
#                         <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
#                         </audio>
#                     """
#                     st.markdown(audio_html, unsafe_allow_html=True)
#
#                 # Ø§Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø¨Ø¹Ø¯ Ù…Ø§ Streamlit ÙŠØ³ØªØ®Ø¯Ù…Ù‡
#                 os.remove(tmp_path)
#
#             except Exception as e:
#                 st.error(f"âš  Ø­ØµÙ„Øª Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ù†Ø·Ù‚: {str(e)}")
#     else:
#         st.warning("âŒ No objects detected.")